### Coursera - Shallow Neural Network (week3)

-유료 결제를 하였습니다.....!!! 과제들은 다음주에 정리해서 올리도록 하겠습니다!


# 1. Recap

 먼저 지난주 했던 것을 간단히 돌아보고, 잘 봐두어야 할 것들을 짚고 넘어가겠습니다. 
 먼저 대략적으로 Neural Network에 대해 정리하면 아래와 같습니다.
 ![Imgur](https://i.imgur.com/fTzSvoh.png)
 ![Imgur](https://i.imgur.com/wt08cUd.png)
 여기서 [i]의 표기는 i번째 layer라는 뜻이고, dw와 같은 표기는 loss 함수를 w로 미분한 것입니다.
 ![Imgur](https://i.imgur.com/YvOxznR.png)
 
 지난 주에 가장 간단한 Neural Network로 Logistic Regression(RL)을 예시로 공부하였습니다. 또한 신경망을 학습하는 방법으로 Gradient Descent가 있었고, 이를 효율적으로 하기 위해 Vectorization을 배웠습니다. RL의 Vectorization을 정리하면 아래와 같습니다. 
![Imgur](https://i.imgur.com/7tjLFN8.png)
쉽게 설명하기 위해 b를 zero vector로 한 것입니다.
![Imgur](https://i.imgur.com/J7qfpuU.png)
 (i)의 표기는 i번째 training data set라는 것을 뜻합니다.
  따라서 input layer인 x는 a[0]와 같고, output인 yhat은 a[2]와 같습니다.
  여기서 기억해야 할 것은, 
   그리고 관습적으로 Neural Network가 몇개의 layer로 이루어져있는지 표현할 때 input layer은 제외하고 표현한다고 합니다. 즉 위의 그림은 2개의 layer로 이루어진 Neural Network입니다.
   
Back propagation을 이용하여 Gradient를 구하여 신경망을 학습시키는 방법을 정리하면 아래와 같습니다.
 ![Imgur](https://i.imgur.com/wDI3WLH.png)
![Imgur](https://i.imgur.com/RoTE7U6.png)
 여기서 기억해야 할 것은 dz를 기본적으로 구해야 dw, db를 구할 수 있다는 것입니다. dw = xdz, db = dz의 식과 왜 이런식이 나오는지(Chain Rule), Vectorization을 복습하시면 이후의 내용들이 훨씬 수월해 질 것입니다. 
 오른쪽의 vectorization을 이용한 식도 기억해두어야 하는데 여기선 w가 열벡터이지만 recap이 끝나고 본내용에서는 w가 행벡터일때가 있습니다. 그러면 dw식을 transpose하기만 하면 됩니다. 즉 dw = dz XT가 됩니다. 이는 아래서 다시 보겠습니다.
 
  위의 두 내용을 한번에 정리하면 아래와 같습니다.
![Imgur](https://i.imgur.com/WsR2ZpM.png)
----

# 2. Shallow Neural Network

 이제 week3의 내용을 처음부터 살펴보겠습니다.
![Imgur](https://i.imgur.com/jx9tIwL.png)
 뉴런 하나를 살펴보면 선형적인 식(WTx+b)과 이를 비선형으로 만들어주는 activating function(σ(x))으로 이루어져 있습니다. 여기서 RL이므로 activating function은 sigmoid function입니다. 즉 4개의 unit이 있으므로 w도 4개, b도 4개가 있습니다. x가 3개로 이루어져 있으므로 W는 4*3개의 성분으로 이루어져 있는 행렬이 됩니다. 위의 그림에서는 w를 각 노드의 wi(i번째 노드의 w -> 열벡터)를 가로로 나열한 행렬로 정의했습니다.

## Activating Function

Activation function에는 여러가지가 있는데, 대표적인 몇가지를 살펴 보겠습니다.
![Imgur](https://i.imgur.com/hQ4iGk0.png)
 Sigmoid function은 RL에서 output layer에서 사용하고, 그 이외에는 잘 사용하지 않습니다. 그 이유는 z가 너무 작거나 크면 기울기가 0에 가까워 지게 됩니다. 그러면 신경망의 학습이 너무 느려지게 됩니다. Gradient가 너무 작으니 update도 작게 되기 때문입니다. 따라서 대부분 RL에서 output layer에서만 사용합니다.
  Sigmoid function의 장점은 미분값을 구할때 함수값을 이용할 수 있다는 것입니다. 도함수가 a(1-a)이기 때문입니다.
   ![Imgur](https://i.imgur.com/mEN8XMl.png)
  hyperbolic tangent function(Tanh function)의 식은 위와 같다. Tanh function은 거의 항상 sigmoid보다 좋은 성능을 낸다. 그 이유는 함수값의 평균이 0에 가깝기 때문이다.(나중에 다룸)
  ![Imgur](https://i.imgur.com/LiPmEC1.png)
   Activation function으로 가장 많이 쓰이는 것이 ReLU입니다. 함수는 위와 같습니다. z가 0일때 미분불가능하지만 보통 0이라고 합니다.(구글링) 1이라고 해도 상관이 없는데 그 이유는 z값이 작으면 어차피 gradient가 매우 작이지므로 ( dw = xdz에서 x의 역할) 크게 영향을 받지 않기 때문이라고 합니다.
    ReLU를 약간 변형한 것이 Leaky ReLU입니다. 식은 위와 같습니다. z가 음수가 될때 unit들이 활성화 안되는 것을 바꿔주기 위해서입니다.
   ![Imgur](https://i.imgur.com/qKJ81IM.png)
  그럼 왜 activation function은 왜 non-linear이어야 하는지 알아 보도록 하겠습니다. 만약 activation function이 선형이면 이는 activation function이 없는 것과 같습니다. 단순 k배에 상수를 더하는 것은 W와 b로 해줄수 있기 때문입니다. 그러므로 위에서 볼 수 있듯이 input과 output은 단순한 선형관계가 되어, 선형회귀가 되어버립니다. 어려층의 layer도 필요가 없어지게 됩니다. 이와 같은 이유로 activation function으로 non-linear를 쓰는 것입니다.

## Gradient Descent

  이제 2 layer RL Neural Network의 Gradient Descent(GD)에 대해서 보겠습니다.
![Imgur](https://i.imgur.com/vytoMqv.png)
 Recap에서 언급했듯이 기본은 dz를 구하는 것입니다.
 
 [2] layer(여기선 마지막 layer, 즉 output layer)부터는 단층 RL Neural Network의 GD와 식이 같습니다. 한가지 다른점은 dW[2]의 경우 행벡터라 dw = adz의 식을 transpose 해준 dw = dzaT의 식을 만족시킨다는 것입니다. 
 [1] layer의 미분값은 chain rule을 사용하여 앞에서 구한 [2] layer의 미분값들 이용하면 구할 수 있습니다. 또한 행렬의 성분 하나에 대해서 생각하면 좀 더 쉽게 생각할 수 있습니다. 예를들어 dz[1] = da[1]/dz[1] * dz[1]/da[2] * dz[2] = g'(z[1])*w[2]*dz[2] 이고 (recap에서 다루었습니다.)이를 행렬로 나타내면  WT[2]dz[1]* g'(z[1]) 가 됩니다.(g는 activation function을 뜻합니다. *는 element wise product를 뜻합니다 .) 
  여기서 element wise product는 같은 위치의 성분끼리 곱하는 것을 뜻합니다. 예를 들어 [1,2]*[2,3] = [2, 6]입니다.
  
  parameter들의 shape는 위에서 확인할 수 있습니다. 행렬의 연산에 관한부분이니 넘어가겠습니다. 하지만 이는 매우 중요한 부분이라 꼭 알고 넘어가야 하는 내용입니다.

![Imgur](https://i.imgur.com/CBPANqY.png)

GD를 요약하면 위와 같습니다. numpy를 쓰는 부분은 다음주에 정리하겠습니다. 여기서는 np.sum은 성분을 더해준다라는 정도만 알면 될 것 같습니다.

## Initialization
 parameter를 update하려면 초기값을 지정해주어야 하는데, 편하게 모두 0으로 시작하면 안될까? 라는 생각을 할 수 있습니다. 결론은 안된다!입니다. 그 이유는 모든 unit의 파라미터가 0이면 unit이 여러개가 필요하지 않게 됩니다. 그냥 unit한개와 0인 W(parameter) 한개만 있어도 같은 작용을 합니다. ReLU를 사용할 경우 학습도 되지 않습니다.

![Imgur](https://i.imgur.com/BlA8WMv.png)

따라서 random하게 파라미터를 초기화합니다. 여기선 정규분포에서 값을 뽑아서 0.01을 곱해줬습니다. 0.01을 곱하는 이유는 파라미터를 작게 하기위해서인데, sigmoid나 Tanh 함수는 z가 조금만 크거나 작아져도 기울기가 0에 가까워져 모델의 학습이 느려지게 됩니다. 따라서 parameter를 작게해서 z값을 너무 크지 않게 하기 위하여 0.01을 곱하는 것입니다.
![Imgur](https://i.imgur.com/zJe45lD.png)

# 3. 궁금점

1. activation function으로 ReLU를 사용할 때, z가 음수가 되면 a= g(z) = 0 이므로 dw = adz =0이 되므로 해당 layer 이전의 layer에서는 parameter update가 안되는 것인가?
 2. 만약 초기화가 잘못되어서 바로 z가 0보다 작게되어버리면 유닛의 parameter는 update가 계속 안되는 것(또는 활성화가 안됨)인가? 
 3. 그럼 모델의 성능에 큰 문제가 생길까? 
