### Coursera - Neural Networks and Deep Learning (week2)


# 1. Logistic Regression as a Neural Network


![Imgur](https://i.imgur.com/k9UzJm7.png)
Logistic Regression은 Classification에서 많이 쓰입니다. 위의 예처럼, 주어진 사진이 고양이 사진인지 분류하는 Binary Classification 문제를 보겠습니다. 사진은 RGB픽셀로 이루어져 있습니다. 위의 사진이 64*64*3픽셀이라 하겠습니다. 그림은 간단하게 보여주기 위하여 4*4*3으로 하였습니다. 이 모든 픽셀값들을 이용하여 Logistic Regression(줄여서 LR이라 하겠습니다.)을 하려 합니다. LR에서 Input 데이터의 Shape는 열벡터이어야 합니다. 따라서 픽셀값들을 수평으로 나열합니다. (추후에 CNN에서는 이 것을 Flattening이라 합니다.) 이렇게 만든 X는 12288차원의 열벡터과 되고, 이를 입력하여 LR을 통해 Y를 출력합니다.

![Imgur](https://i.imgur.com/4NiRs5j.png)
먼저 Logistic Regression에서 사용할 Notation들을 정리하하겠습니다. 입력값 x는 nx차원의 열벡터입니다. y는 1 또는 0(고양인지 아닌지)입니다. m개의 training example이 있으므로 m개의 (x(i), y(i)) 데이터가 있습니다. X는 m개의 x(i)를 수평으로 묶은 벡터입니다. 따라서 X는 nx*b(nx,b)의 shape를 갖습니다. Y는 m개의 y(i)를 수평으로 묶은 행벡터입니다.

![Imgur](https://i.imgur.com/GFi5sJZ.png)
  LR에서 얻고자 하는 값은 y hat입니다. 즉 x일 때 y가 1이 될 확률입니다. 이 확률로 우리는 y가 1 또는 0일지 판단할 것입니다. 
  LR에는 두 개의 Parameter가 있습니다. W와 b입니다. LR이 주어진 데이터에서 좋은 결과를 내도록 W와 b를 update할 것 입니다. w는 x와 같은 차원의 열벡터입니다. b는 상수입니다. w와 x를 내적(wT.x)하고 b를 더한 값은 0과 1사이의 값이 아닐 수 있습니다. sigmoid 함수를 사용하여 이 값을 0과 1사이의 값으로 만듭니다. 
   sigmoid함수는  σ(x)라 쓰고, 식은 위의 사진에 나와 있습니다. 증가함수이며 (0, 0.5)에 대칭이고, y값은 x가 작아지면 0에 근사하고, 커지면 1에 근사합니다. 

![Imgur](https://i.imgur.com/xUJMlcD.png)
 LR에서 cost function과 loss function에 대해 알아보겠습니다.
 x값을 입력해 y hat을 구하면 실제 y값과의 차이(error)가 생길것 입니다. 이 error를 나타내는 것이 cost function과 loss function입니다. 따라서 이 함수의 값이 작으면 작을수록 LR의 성능이 좋다고 할 수 있겠죠. 그래서 이 함수들은 후에 LR에서는 cost function을 최소화하도록 두개의 parameter인 w,b를 update합니다.
  loss function은 하나의 데이터에 대한 error를 나타내는 것이고, cost function은 전체(m개)의 training data에 대한 error입니다. 즉 모든 데이터의 loss function의 평균이 cost function입니다. 
   고전적인 loss function은 (y-y hat)^2인데 LR에선 모양이 많이 다른 것을 알수 있습니다. LR의 cost function같은 형태를 negative log function이라 합니다. 왜 이러한 함수를 쓰냐면 이 함수는 convex모양(민무늬 토기)이기 때문입니다. 여러 극값(local minimum)을 가지면 최적의 w,b로 update하기가 어렵기 때문입니다.  그럼 아래에서 LR의 cost function에 대해 좀더 알아보겠습니다.
   ![Imgur](https://i.imgur.com/hb8pyK0.png)
   위를 보면 이러한 cost function의 값이 작을수록 error가 작아지는지 확인할 수 있습니다. y가 1이라면 y hat이 1에 가까워 져야하고, y가 0이라면 y hat은 0에 가까워져야 합니다.(LR은 지도학습이므로 y는 주어집니다.)

----
 그럼이제 w,b를 update하는 방법인 gradient descent에 대해 알아보겠습니다.
 ![Imgur](https://i.imgur.com/28Dowoj.png)
  우선 LR의 cost function 그래프를 그려보면 왼쪽위의 3차원 그래프(convex)입니다. 더 높은 차원이겠지만 시각화를 위하여 3차원으로 표현하였습니다.
   J(w,b)는 데이터가 주어졌을 때 parameter w, b에 대한 cost function입니다. 이 함수값을 최소화 하는 w, b를 찾는 것이 목표입니다.

![Imgur](https://i.imgur.com/hXG3aKl.png)
 위는 cost function을 2차원으로 표현한 것입니다.
 Gradient란 경사라 생각면 됩니다. 특정 w일 때의 J(w,b)의 경사(기울기)를 구하고 그 경사를 따라 내려오도록 w를 update 합니다. b도 마찬가지로 update를 합니다. 이 것을 gradient descent (최대경사하강법)이라고 합니다. 즉 특정 parameter에서 cost function의 minimum으로 가는 가장 가파른 경사를 갖는 길을 찾고, 그 길을 따라 (Learning rate - α)만큼 내려가는(w, b를 update) 것입니다.
  그럼 최대 경사는 어떻게 구하는지 알아보겠습니다.
  J(w,b)를 w와 b로 편미분하면 끝!
  여기서 기억해야 할 것은 J(w, b)를 w로 편미분한 것(경사)를 dw, b로 편미분 한것을 db같이 표기하는 약속입니다. z로 편미분 하면 dz겠죠?
  
  ----
![Imgur](https://i.imgur.com/9bVzSOH.png)
그럼 이제 이 미분을 컴퓨터로 어떻게 구하는지 알아보겠습니다. 가장 중요한건 chain rule입니다! 자세한 설명은 생략하겠습니다. 위를 참고해 주세요! 
![Imgur](https://i.imgur.com/kp5YtFT.png)
 이를 LR에서 일반적으로 나타내면 위와 같습니다. sigmoid함수를 미분한 결과도 나와 있는데 모르시면 패스! 고등학교 이과에서는 다루는 내용이라 넘어가겠습니다. 
 여기서 결론적으로 기억하셔야 할 것은 dw(i) = x(i)dz, db = dz입니다.
![Imgur](https://i.imgur.com/SrsBkdo.png)

 loss fucntion을 미분하는 법을 알았으니, cost function(J(w,b))도 미분할 수 있습니다..! loss를 평균내면 cost이니까요! 위를 참고하시면 됩니다! 
 즉 cost를 미분한 것은, 각 데이터별로 loss의 경사(gradient)를 구해서(미분) 평균을 내면됩니다. cost의 경사를 구하는 거죠!!


![Imgur](https://i.imgur.com/r2jEHU9.png)
 그럼 lost function 미분을 computing하는 과정을 살펴보겠습니다. 총 m개의 데이터가 있으가 m번의 loss를 구해서(m번의 loop) 계속 더해가고 마지막에 m으로 나누어 평균을 구하면 끝!
  loop가 많을 수록 computing시간은 길어지고 학습속도는 느려지게 됩니다. 따라서 loop문을 최소한으로 이용하기 위해 vectorizing을 이용할 것 입니다.
   그럼 이제 vectorizing을 살펴보겠습니다.


